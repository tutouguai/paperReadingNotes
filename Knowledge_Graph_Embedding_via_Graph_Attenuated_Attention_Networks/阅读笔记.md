# 通过图形衰减的注意力网络进行知识图谱嵌入

## 摘要

图卷积神经网络在知识图的<b>关系路径</b>上分配相同的权重，忽略了相邻节点中丰富的信息，导致了三重特征的不完全挖掘。为此，我们提出了一种新的表示方法——衰减注意网络（GAAT），它集成了一种衰减注意机制，在不同的关系路径中分配不同的权重，并从邻域中获取信息。因此，实体和关系可以在任何邻居中学习。

## 1. 背景介绍和论文主要工作

模型引入了衰减注意机制，同时考虑了n-跳邻居(<b>e实体离给定实体越近，获得的注意力的权重就越高</b>)模型分别训练关系嵌入和实体嵌入。

### 1.1主要工作

- 引入了图注意力网络
- 针对图卷积神经网络的局限性，提出了衰减注意力机制，并提出了一种基于图衰减注意力网络的实体和关系的嵌入方法
- 利用n跳邻居节点的信息来拓展实体和关系的表示
- 使用图注意力衰减网络作为编码器，使用胶囊网络嵌入作为解码器(CapsE:模型支持在更深层次上探索三重特征)

## 2.相关工作介绍

### 2.1张量分解

​		张量分解的基本思想是用多个低维矩阵或张量(二维矩阵)替代原始关系矩阵，从而用少量参数替换稀疏的大量原始数据。(tucker论文-未读)

​		1. RESCAL(<b>A three-way model for collective learning on multi-relational data</b>)通过张量因子分解，考虑了二元关系数据的固有结构。

​		2. 神经张量网络(ntn: <b>Reasoning with neural tensor networks for knowledge base completion</b>)模型将关系表示为矩阵，以表征潜在特征的相关性。实体与关系之间的双线性匹配用于判断建立关系的可能性。

### 2.2 知识嵌入模型

#### 	2.2.1 平移模型

​	平移模型是基于能量函数的，正确三元组的能量较低，损坏三元组的能量较高。已生成的三元组是否为正确三元组的判断局域能量函数的计算。

 1. TransE 是一个参数较少的简单模型，但在处理一对多和多对一关系时存在一定问题。

    (<b>Translating embeddings for modeling multi-relational data</b>)

 2. 为了解决TransE的问题，提出了超平面上的平移嵌入(TransH: <b>Knowledge graph embedding by translating on hyperplanes</b>>)和关系空间中的平移嵌入(TransR: <b>Learning entity and relation embeddings for knowledge graph completion</b>),TransH和TransR实验在关系空间中的替代表示法计算统一试题的分数，有效避免了收敛问题。

 3. 动态映射矩阵（TransD:）转换嵌入应用由相应实体和关系确定的过渡矩阵，解决了实体和关系的多样性。

    #### 2.2.2 神经网络模型

    ​	知识表示是指如何通过神经网络模型表达实体和关系，从而通过符号计算实体和关系。更注重三元组潜在语义信息。

     1. 神经张量模型(NTN)

     2. 卷积知识库嵌入(ConvKB: <b>A novel embedding model for knowledge base completion based on convolutional neural network</b>)

        ConvKB使用文本关系的内部结构作为卷积神经网络的输入

     3. 卷积网络嵌入(ConvE: <b>Convolutional 2D knowledge graph embeddings</b>)

        使用卷积神经网络（CNN）优化输入向量以对三元组进行评分

     4. 关系图卷积网络(R-GCN: <b>Modeling relational data with graph convolutional networks</b>)

        图卷积网络获得三元组的嵌入，然后应用DistMult(<b>Embedding entities and relations for learning and inference in knowledge bases</b>)计算嵌入的分数。

     5. 图卷积网络(GCNs: <b>Semisupervised classifification with graph convolutional networks</b>)

     6. 胶囊网络嵌入(CapsE:  <b>A capsule network-based embedding model for knowledge graph completion and search personalization</b>)


### 2.3 全息嵌入模型

​	HOLE: <b>Holographic embeddings of knowledge graphs</b>

​	前两种模型的计算量大，为了解决这一限制，提出了全息嵌入模型。通过应用实体嵌入的循环相关来构造更有效的嵌入表示。

### 2.4 路径学习

#### 	2.4.1 RL

​		<b>Multi–hop knowledge graph reasoning with reward shaping</b>

​		基于多跳知识嵌入的强化学习用于基于知识图的查询应答

#### 	2.4.2 关系路径嵌入(RPE)

​		<b>Relation path embedding in knowledge graphs</b>

​		关系路径嵌入将多跳关系路径添加到转换模型中，同时将每个实体嵌入到两种类型的潜在空间中。

#### 	2.4.3 PARL

​		<b>Path-based attribute-aware representation learning for relation prediction</b>

​		基于路径的实现感知表示学习模型(PARL),对关系预测任务进行路径去噪和路径表示学习。

#### 	2.4.4 DPTransE

​		<b>Discriminative path-based knowledge graph embedding for precise link prediction</b>

​		基于辨别路径的嵌入模型，从联合学习的潜在特征和图特征构建交互，并使用图形特征作为提供精确特征和辨别行嵌入的关键先决条件。

#### 	2.4.5 Meta_KGR

​		<b>Adapting meta knowledge graph information for multi–hop reasoning over few–shot relations</b>

​		基于元的多跳知识图推理，采用元学习，从高频关系中学习有效的元参数，可以适应寥寥无几的关系

### 2.5 注意力机制

​	注意力机制与神经网络模型的结合也得到了广泛的关注，其目的是提高模型的鲁棒性。

#### 	2.5.1 注意力图卷积网络(AGCN)

​		<b>Attention graph convolution network for image segmentation in big SAR imagery data</b>

​		由注意机制层和图卷积神经网络(GCN)组成，用于在大SAP(<b>???</b>)图像数据中执行超像素分割。

#### 	2.5.2 图注意力模型(GAM)

​		<b>Graph classifification using structural attention</b>

​		关注图形中较小但信息丰富的部分，避免图形其余部分的噪声。

### 2.6 其它

#### 	2.6.1 KPRN

​		<b>Explainable reasoning over knowledge graphs for recommendation</b>

​		知识感知路径递归网络（KPRN）通过组合实体和关系的语义生成路径表示，并允许对路径进行有效推理，以推断用户项交互的基本原理，以进行推荐。

#### 2.6.2KGAT

​		<b>KGAT: Knowledge graph attention network for recommendation</b>

​		知识图注意网络(KGAT)递归的传播来自节点邻居的嵌入一细化节点嵌入，并使用注意力机制来区分邻居的重要性以进行推荐。

### 2.7 总结

​	但是现有的模型只考虑关系嵌入作为实体嵌入的辅助特征，并没有深入地考虑如何嵌入关系。此外，当两个实体之间存在间接连接（不是一跳关系）时，以前的研究将相同的权重分配给每个跳的关系，导致部分相邻三元组的信息丢失。目前，文献中还没有集成神经网络模型和平移模型的模型，利用了这两种模型的优点。在这里，我们提出了一个组合模型，<b>以进一步提高任务的知识图完成方面的预测精度和计算成本(提高计算成本????)</b>。



## 3.方法

 节点的注意值可以形式化为 ${\large e_{ij}=f_{a}(W_{e_{i},W_{e_{j}}})}$ 

提出了一个参数线性变换矩阵，将输入特征映射到高维输出特征空间。

